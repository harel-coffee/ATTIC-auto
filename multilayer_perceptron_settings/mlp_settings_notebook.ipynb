{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "from joblib import dump, load\n",
    "#from nltk import SnowballStemmer\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy\n",
    "#!{sys.executable} -m pip install numpy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#!{sys.executable} -m pip install sklearn\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "boost_summary = 3\n",
    "project_keys = [\"HTTPCLIENT\", \"LUCENE\", \"JCR\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    raw_data = []\n",
    "    data_directory = \"..\" + os.path.sep + \"data\"\n",
    "    for filename in os.listdir(data_directory):\n",
    "        with codecs.open(data_directory + os.path.sep + filename, \"r\", \"utf-8\") as fin:\n",
    "            raw_data += json.load(fin)\n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_corpus_labels(raw_data, stemmer=None):\n",
    "    # Corpus building.\n",
    "    corpus = []\n",
    "    labels = []\n",
    "    n_bug = 0\n",
    "    for n_file in raw_data:\n",
    "\n",
    "        txt = \"\"\n",
    "        for i in range(boost_summary):\n",
    "            txt += n_file[\"summary\"] + \" \"\n",
    "            if stemmer is not None:\n",
    "                txt = stemming_textual_data(stemmer,txt)\n",
    "\n",
    "        if stemmer is not None:\n",
    "            n_file[\"description\"] = stemming_textual_data(stemmer,n_file[\"description\"])\n",
    "\n",
    "        corpus.append(txt + \" \" + n_file[\"description\"])\n",
    "        labels.append(n_file[\"label\"])\n",
    "        if n_file[\"label\"] == \"BUG\":\n",
    "            n_bug += 1\n",
    "    print(f\"{n_bug} BUG / {len(labels)} \\n\")\n",
    "    return corpus, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def stemming_textual_data(stemmer, textual_data):\n",
    "    return ' '.join([stemmer.stem(word) for word in textual_data.split(' ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def feature_computing(corpus, labels, vectorizer, feature_selection = True, k_best=30000):\n",
    "    # TF-IDF.\n",
    "    print(\"Feature computing.\")\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    print(f\"\\t{X.shape[1]} features.\")\n",
    "\n",
    "    if feature_selection:\n",
    "        print(\"Extracting %d best features by a chi-squared test\" % k_best)\n",
    "        ch2 = SelectKBest(chi2, k=k_best)\n",
    "        X = ch2.fit_transform(X, labels)\n",
    "\n",
    "        #if feature_names:  # keep selected feature names.\n",
    "        #    feature_names = [feature_names[i] for i in ch2.get_support(indices=True)]\n",
    "        return X, vectorizer, ch2\n",
    "\n",
    "\n",
    "    return X, vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Split data by project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def split_data_by_project(raw_data, stemmer=None):\n",
    "    # Create dicts of tickets for each project\n",
    "    dict_data_split = {}\n",
    "    print(\"Split data for each project\")\n",
    "    for project_key in project_keys:\n",
    "        dict_data_split[project_key] = {}\n",
    "        dict_data_split[project_key][\"tickets\"] = []\n",
    "        dict_data_split[project_key][\"corpus\"] = []\n",
    "        dict_data_split[project_key][\"labels\"] = []\n",
    "\n",
    "    for ticket in raw_data:\n",
    "        for project_key in project_keys:\n",
    "            if project_key in ticket[\"key\"]:\n",
    "                dict_data_split[project_key][\"tickets\"].append(ticket)\n",
    "\n",
    "    for project_key in project_keys:\n",
    "        print(\"Get corpus and labels for project: \", project_key)\n",
    "        tickets = dict_data_split[project_key][\"tickets\"]\n",
    "        # Get corpus and labels for specific project tickets\n",
    "        if stemmer is not None:\n",
    "            corpus, labels = get_corpus_labels(tickets,stemmer)\n",
    "        else:\n",
    "            corpus, labels = get_corpus_labels(tickets)\n",
    "        dict_data_split[project_key][\"corpus\"] = corpus\n",
    "        dict_data_split[project_key][\"labels\"] = labels\n",
    "\n",
    "    return dict_data_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def labels_binarizing(labels):\n",
    "    lb = LabelBinarizer()\n",
    "    # Binarize labels with BUG = 0 and NBUG = 1\n",
    "    labels = numpy.array([number[0] for number in lb.fit_transform(labels)])\n",
    "    # Inverse 0 and 1 to have good labels, i.e BUG = 1 and NBUG = 0\n",
    "    return numpy.logical_not(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def make_scoring(X, binarized_labels, clf, cv=10):\n",
    "    #scores = cross_val_score(clf, X, binarized_labels, cv=cv, scoring='f1')\n",
    "    #print(\"F1score: %0.3f\" % scores.mean())\n",
    "    #print(\"95%% Confidence Interval +/- %0.3f\" % (scores.std() * 2))\n",
    "    #print(\"Standard deviation: %0.3f\\n\" % scores.std())\n",
    "\n",
    "    scores = cross_val_score(clf, X, binarized_labels, cv=cv, scoring='precision')\n",
    "    print(\"Precision: %0.3f\" % scores.mean())\n",
    "    print(\"95%% Confidence Interval +/- %0.3f\" % (scores.std() * 2))\n",
    "    print(\"Standard deviation: %0.3f\\n\" % scores.std())\n",
    "\n",
    "    scores = cross_val_score(clf, X, binarized_labels, cv=cv, scoring='recall')\n",
    "    print(\"Recall: %0.3f\" % scores.mean())\n",
    "    print(\"95%% Confidence Interval +/- %0.3f\" % (scores.std() * 2))\n",
    "    print(\"Standard deviation: %0.3f\\n\" % scores.std())\n",
    "\n",
    "    #scores = cross_val_score(clf, X, binarized_labels, cv=cv, scoring='accuracy')\n",
    "    #print(\"Accuracy: %0.3f\" % scores.mean())\n",
    "    #print(\"95%% Confidence Interval +/- %0.3f\" % (scores.std() * 2))\n",
    "    #print(\"Standard deviation: %0.3f\\n\" % scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting 1\n",
    "\n",
    "Most raw setting.\n",
    "- Use only uni-grams\n",
    "- No feature selection\n",
    "- Multi-Layer Perceptron set as default\n",
    "- No optimizations\n",
    "\n",
    "Parameters: \n",
    "* No feature selection\n",
    "* TfidfVectorizer(ngram_range=(1, 1), sublinear_tf=False)\n",
    "* MLPClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def run_setting_1(raw_data):\n",
    "    print(\"=> Run setting 1 <=\")\n",
    "    print(\"Get corpus and labels for all projects\")\n",
    "    corpus, labels = get_corpus_labels(raw_data)\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 1), sublinear_tf=False)\n",
    "    mlp = MLPClassifier(random_state=0)\n",
    "    X, vectorizer = feature_computing(corpus, labels, vectorizer, feature_selection=False)\n",
    "    binarized_labels = labels_binarizing(labels)\n",
    "    dict_data_split = split_data_by_project(raw_data)\n",
    "\n",
    "    print(\"=====> Scoring Cross project <=====\")\n",
    "    make_scoring(X,binarized_labels,mlp)\n",
    "\n",
    "    for key in project_keys:\n",
    "        corpus = dict_data_split[key][\"corpus\"]\n",
    "        labels = dict_data_split[key][\"labels\"]\n",
    "\n",
    "        binarized_labels = labels_binarizing(labels)\n",
    "\n",
    "        X = vectorizer.transform(corpus)\n",
    "        print(\"=====> Scoring \" + key + \" <=====\")\n",
    "        make_scoring(X,binarized_labels,mlp)\n",
    "    print(\"=> End run setting 1 <=\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run setting 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Run setting 1 <=\n",
      "Get corpus and labels for all projects\n",
      "1940 BUG / 5591 \n",
      "\n",
      "Feature computing.\n",
      "\t27033 features.\n",
      "Split data for each project\n",
      "Get corpus and labels for project:  HTTPCLIENT\n",
      "305 BUG / 746 \n",
      "\n",
      "Get corpus and labels for project:  LUCENE\n",
      "697 BUG / 2443 \n",
      "\n",
      "Get corpus and labels for project:  JCR\n",
      "938 BUG / 2402 \n",
      "\n",
      "=====> Scoring Cross project <=====\n",
      "F1score: 0.748\n",
      "95% Confidence Interval +/- 0.058\n",
      "Standard deviation: 0.029\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data = load_data()\n",
    "run_setting_1(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting 2\n",
    "\n",
    "Setting using only corpus processing methods i.e, TF-Idf and Chi-2 for feature selection.\n",
    "- Uni-grams and Bi-grams + Rare/Stop words dropping + Log function on term-frequency\n",
    "- No Feature selection\n",
    "- Multi-Layer Perceptron set as default\n",
    "- No optimizations\n",
    "\n",
    "Parameters:\n",
    "* TfidfVectorizer(max_df=0.5, min_df=2, ngram_range=(1, 3), stop_words={\"english\"}, sublinear_tf=True)\n",
    "* MLPClassifier(activation='tanh', learning_rate='adaptive', max_iter=100, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def run_setting_2(raw_data):\n",
    "    print(\"=> Run setting 2 <=\")\n",
    "    print(\"Get corpus and labels for all projects\")\n",
    "    corpus, labels = get_corpus_labels(raw_data)\n",
    "    vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, ngram_range=(1, 2), stop_words={\"english\"},  sublinear_tf=True)\n",
    "    mlp = MLPClassifier(random_state=0)\n",
    "    X, vectorizer = feature_computing(corpus, labels, vectorizer, feature_selection=False)\n",
    "    binarized_labels = labels_binarizing(labels)\n",
    "    dict_data_split = split_data_by_project(raw_data)\n",
    "\n",
    "    print(\"=====> Scoring Cross project <=====\")\n",
    "    make_scoring(X,binarized_labels,mlp)\n",
    "\n",
    "    for key in project_keys:\n",
    "        corpus = dict_data_split[key][\"corpus\"]\n",
    "        labels = dict_data_split[key][\"labels\"]\n",
    "\n",
    "        binarized_labels = labels_binarizing(labels)\n",
    "\n",
    "        X = vectorizer.transform(corpus)\n",
    "        print(\"=====> Scoring \" + key + \" <=====\")\n",
    "        make_scoring(X,binarized_labels,mlp)\n",
    "    print(\"=> End run setting 2 <=\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run setting 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Run setting 2 <=\n",
      "Get corpus and labels for all projects\n",
      "1940 BUG / 5591 \n",
      "\n",
      "Feature computing.\n",
      "\t63924 features.\n",
      "Split data for each project\n",
      "Get corpus and labels for project:  HTTPCLIENT\n",
      "305 BUG / 746 \n",
      "\n",
      "Get corpus and labels for project:  LUCENE\n",
      "697 BUG / 2443 \n",
      "\n",
      "Get corpus and labels for project:  JCR\n",
      "938 BUG / 2402 \n",
      "\n",
      "=====> Scoring Cross project <=====\n",
      "F1score: 0.774\n",
      "95% Confidence Interval +/- 0.035\n",
      "Standard deviation: 0.018\n",
      "\n",
      "Precision: 0.816\n",
      "95% Confidence Interval +/- 0.133\n",
      "Standard deviation: 0.066\n",
      "\n",
      "Recall: 0.743\n",
      "95% Confidence Interval +/- 0.102\n",
      "Standard deviation: 0.051\n",
      "\n",
      "Accuracy: 0.849\n",
      "95% Confidence Interval +/- 0.038\n",
      "Standard deviation: 0.019\n",
      "\n",
      "=====> Scoring HTTPCLIENT <=====\n",
      "F1score: 0.713\n",
      "95% Confidence Interval +/- 0.100\n",
      "Standard deviation: 0.050\n",
      "\n",
      "Precision: 0.738\n",
      "95% Confidence Interval +/- 0.118\n",
      "Standard deviation: 0.059\n",
      "\n",
      "Recall: 0.692\n",
      "95% Confidence Interval +/- 0.123\n",
      "Standard deviation: 0.062\n",
      "\n",
      "Accuracy: 0.772\n",
      "95% Confidence Interval +/- 0.082\n",
      "Standard deviation: 0.041\n",
      "\n",
      "=====> Scoring LUCENE <=====\n",
      "F1score: 0.741\n",
      "95% Confidence Interval +/- 0.112\n",
      "Standard deviation: 0.056\n",
      "\n",
      "Precision: 0.841\n",
      "95% Confidence Interval +/- 0.096\n",
      "Standard deviation: 0.048\n",
      "\n",
      "Recall: 0.666\n",
      "95% Confidence Interval +/- 0.150\n",
      "Standard deviation: 0.075\n",
      "\n",
      "Accuracy: 0.869\n",
      "95% Confidence Interval +/- 0.050\n",
      "Standard deviation: 0.025\n",
      "\n",
      "=====> Scoring JCR <=====\n",
      "F1score: 0.782\n",
      "95% Confidence Interval +/- 0.059\n",
      "Standard deviation: 0.029\n",
      "\n",
      "Precision: 0.813\n",
      "95% Confidence Interval +/- 0.079\n",
      "Standard deviation: 0.039\n",
      "\n",
      "Recall: 0.755\n",
      "95% Confidence Interval +/- 0.069\n",
      "Standard deviation: 0.035\n",
      "\n",
      "Accuracy: 0.836\n",
      "95% Confidence Interval +/- 0.044\n",
      "Standard deviation: 0.022\n",
      "\n",
      "=> End run setting 2 <=\n"
     ]
    }
   ],
   "source": [
    "raw_data = load_data()\n",
    "run_setting_2(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting 3\n",
    "\n",
    "Setting using only corpus processing methods i.e, TF-Idf and Chi-2 for feature selection.\n",
    "- Uni-grams and Bi-grams + Rare/Stop words dropping + Log function on term-frequency\n",
    "- Feature selection with Chi2 (20000)\n",
    "- Multi-Layer Perceptron set as default\n",
    "- No optimizations\n",
    "\n",
    "Parameters: \n",
    "* Feature selection\n",
    "* 30000 features\n",
    "* TfidfVectorizer(max_df=0.5, min_df=2, ngram_range=(1, 3), stop_words={\"english\"}, sublinear_tf=True)\n",
    "* MLPClassifier(activation='tanh', learning_rate='adaptive', max_iter=100, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def run_setting_3(raw_data):\n",
    "    print(\"=> Run setting 3 <=\")\n",
    "    print(\"Get corpus and labels for all projects\")\n",
    "    corpus, labels = get_corpus_labels(raw_data)\n",
    "    vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, ngram_range=(1, 3), stop_words={\"english\"}, sublinear_tf=True)\n",
    "    mlp = MLPClassifier(random_state=0)\n",
    "    X, vectorizer, ch2 = feature_computing(corpus, labels, vectorizer)\n",
    "    binarized_labels = labels_binarizing(labels)\n",
    "    dict_data_split = split_data_by_project(raw_data)\n",
    "\n",
    "    print(\"=====> Scoring Cross project <=====\")\n",
    "    make_scoring(X,binarized_labels,mlp)\n",
    "\n",
    "    for key in project_keys:\n",
    "        corpus = dict_data_split[key][\"corpus\"]\n",
    "        labels = dict_data_split[key][\"labels\"]\n",
    "\n",
    "        binarized_labels = labels_binarizing(labels)\n",
    "\n",
    "        X = vectorizer.transform(corpus)\n",
    "        X = ch2.transform(X)\n",
    "        print(\"=====> Scoring \" + key + \" <=====\")\n",
    "        make_scoring(X,binarized_labels,mlp)\n",
    "    print(\"=> End run setting 3 <=\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run setting 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Run setting 3 <=\n",
      "Get corpus and labels for all projects\n",
      "1940 BUG / 5591 \n",
      "\n",
      "Feature computing.\n",
      "\t99349 features.\n",
      "Extracting 30000 best features by a chi-squared test\n",
      "Split data for each project\n",
      "Get corpus and labels for project:  HTTPCLIENT\n",
      "305 BUG / 746 \n",
      "\n",
      "Get corpus and labels for project:  LUCENE\n",
      "697 BUG / 2443 \n",
      "\n",
      "Get corpus and labels for project:  JCR\n",
      "938 BUG / 2402 \n",
      "\n",
      "=====> Scoring Cross project <=====\n",
      "F1score: 0.866\n",
      "95% Confidence Interval +/- 0.031\n",
      "Standard deviation: 0.015\n",
      "\n",
      "Precision: 0.929\n",
      "95% Confidence Interval +/- 0.082\n",
      "Standard deviation: 0.041\n",
      "\n",
      "Recall: 0.813\n",
      "95% Confidence Interval +/- 0.073\n",
      "Standard deviation: 0.036\n",
      "\n",
      "Accuracy: 0.913\n",
      "95% Confidence Interval +/- 0.021\n",
      "Standard deviation: 0.010\n",
      "\n",
      "=====> Scoring HTTPCLIENT <=====\n",
      "F1score: 0.761\n",
      "95% Confidence Interval +/- 0.081\n",
      "Standard deviation: 0.041\n",
      "\n",
      "Precision: 0.909\n",
      "95% Confidence Interval +/- 0.142\n",
      "Standard deviation: 0.071\n",
      "\n",
      "Recall: 0.656\n",
      "95% Confidence Interval +/- 0.068\n",
      "Standard deviation: 0.034\n",
      "\n",
      "Accuracy: 0.831\n",
      "95% Confidence Interval +/- 0.061\n",
      "Standard deviation: 0.031\n",
      "\n",
      "=====> Scoring LUCENE <=====\n",
      "F1score: 0.849\n",
      "95% Confidence Interval +/- 0.116\n",
      "Standard deviation: 0.058\n",
      "\n",
      "Precision: 0.945\n",
      "95% Confidence Interval +/- 0.062\n",
      "Standard deviation: 0.031\n",
      "\n",
      "Recall: 0.776\n",
      "95% Confidence Interval +/- 0.188\n",
      "Standard deviation: 0.094\n",
      "\n",
      "Accuracy: 0.923\n",
      "95% Confidence Interval +/- 0.053\n",
      "Standard deviation: 0.027\n",
      "\n",
      "=====> Scoring JCR <=====\n",
      "F1score: 0.862\n",
      "95% Confidence Interval +/- 0.042\n",
      "Standard deviation: 0.021\n",
      "\n",
      "Precision: 0.934\n",
      "95% Confidence Interval +/- 0.059\n",
      "Standard deviation: 0.029\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data = load_data()\n",
    "run_setting_3(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting 4\n",
    "Intermediate setting optimized only with Grid-Search:\n",
    "- Uni-grams and Bi-grams + Rare/Stop words dropping + Log function on term-frequency\n",
    "- A feature number not optimized (20000)\n",
    "- MLP parameters optimized with Grid-Search\n",
    "\n",
    "Parameters\n",
    "* Feature selection\n",
    "* 30000 features\n",
    "* TfidfVectorizer(max_df=0.5, min_df=2, ngram_range=(1, 3), stop_words={\"english\"}, sublinear_tf=True)\n",
    "* MLPClassifier(activation='tanh', learning_rate='adaptive', max_iter=100, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def run_setting_4(raw_data):\n",
    "    print(\"=> Run setting 4 <=\")\n",
    "    print(\"Get corpus and labels for all projects\")\n",
    "    corpus, labels = get_corpus_labels(raw_data)\n",
    "    vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, ngram_range=(1, 3), stop_words={\"english\"}, sublinear_tf=True)\n",
    "    mlp = MLPClassifier(activation='tanh', learning_rate='adaptive', max_iter=100, random_state=0)\n",
    "    X, vectorizer, ch2 = feature_computing(corpus, labels, vectorizer)\n",
    "    binarized_labels = labels_binarizing(labels)\n",
    "    dict_data_split = split_data_by_project(raw_data)\n",
    "\n",
    "    print(\"=====> Scoring Cross project <=====\")\n",
    "    make_scoring(X,binarized_labels,mlp)\n",
    "\n",
    "    for key in project_keys:\n",
    "        corpus = dict_data_split[key][\"corpus\"]\n",
    "        labels = dict_data_split[key][\"labels\"]\n",
    "\n",
    "        binarized_labels = labels_binarizing(labels)\n",
    "\n",
    "        X = vectorizer.transform(corpus)\n",
    "        X = ch2.transform(X)\n",
    "        print(\"=====> Scoring \" + key + \" <=====\")\n",
    "        make_scoring(X,binarized_labels,mlp)\n",
    "    print(\"=> End run setting 4 <=\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run setting 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Run setting 4 <=\n",
      "Get corpus and labels for all projects\n",
      "1940 BUG / 5591 \n",
      "\n",
      "Feature computing.\n",
      "\t99349 features.\n",
      "Extracting 30000 best features by a chi-squared test\n",
      "Split data for each project\n",
      "Get corpus and labels for project:  HTTPCLIENT\n",
      "305 BUG / 746 \n",
      "\n",
      "Get corpus and labels for project:  LUCENE\n",
      "697 BUG / 2443 \n",
      "\n",
      "Get corpus and labels for project:  JCR\n",
      "938 BUG / 2402 \n",
      "\n",
      "=====> Scoring Cross project <=====\n"
     ]
    }
   ],
   "source": [
    "raw_data = load_data()\n",
    "run_setting_4(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Setting 5\n",
    "\n",
    "Setting most optimized (Grid-Search + Genetic Algorithm) using:\n",
    "- Uni-grams and Bi-grams + Rare/Stop words dropping + Log function on term-frequency\n",
    "- A feature number optimized with GA (24954)\n",
    "- MLP parameters optimized with Grid-Search and GA\n",
    "\n",
    "Parameters:\n",
    "* 24954 features\n",
    "* TfidfVectorizer(max_df=0.5, min_df=2, ngram_range=(1, 3), sublinear_tf=True)\n",
    "* MLPClassifier(hidden_layer_sizes=(8, 13, 18, 14, 11), activation='tanh', learning_rate='adaptive', max_iter=100, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def run_setting_5(raw_data):\n",
    "    print(\"=> Run setting 5 <=\")\n",
    "    print(\"Get corpus and labels for all projects\")\n",
    "    #stemmer = SnowballStemmer(\"english\")\n",
    "    stemmer = None\n",
    "    corpus, labels = get_corpus_labels(raw_data,stemmer)\n",
    "    vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, ngram_range=(1, 3), sublinear_tf=True, stop_words={'english'})\n",
    "\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(15, 9, 10, 11, 9, 15, 11), activation='tanh', learning_rate='adaptive',\n",
    "                        max_iter=100, random_state=0)\n",
    "\n",
    "    X, vectorizer, ch2 = feature_computing(corpus, labels, vectorizer, k_best=37362)\n",
    "    binarized_labels = labels_binarizing(labels)\n",
    "    dict_data_split = split_data_by_project(raw_data, stemmer)\n",
    "\n",
    "    print(\"=====> Scoring Cross project <=====\")\n",
    "    make_scoring(X,binarized_labels,mlp)\n",
    "\n",
    "    for key in project_keys:\n",
    "        corpus = dict_data_split[key][\"corpus\"]\n",
    "        labels = dict_data_split[key][\"labels\"]\n",
    "\n",
    "        binarized_labels = labels_binarizing(labels)\n",
    "\n",
    "        X = vectorizer.transform(corpus)\n",
    "        X = ch2.transform(X)\n",
    "        print(\"=====> Scoring \" + key + \" <=====\")\n",
    "        make_scoring(X,binarized_labels,mlp)\n",
    "    print(\"=> End run setting 5 <=\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Run setting 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Run setting 5 <=\n",
      "Get corpus and labels for all projects\n",
      "1940 BUG / 5591 \n",
      "\n",
      "Feature computing.\n",
      "\t99349 features.\n",
      "Extracting 36444 best features by a chi-squared test\n",
      "Split data for each project\n",
      "Get corpus and labels for project:  HTTPCLIENT\n",
      "305 BUG / 746 \n",
      "\n",
      "Get corpus and labels for project:  LUCENE\n",
      "697 BUG / 2443 \n",
      "\n",
      "Get corpus and labels for project:  JCR\n",
      "938 BUG / 2402 \n",
      "\n",
      "=====> Scoring Cross project <=====\n",
      "F1score: 0.895\n",
      "95% Confidence Interval +/- 0.036\n",
      "Standard deviation: 0.018\n",
      "\n",
      "Precision: 0.912\n",
      "95% Confidence Interval +/- 0.098\n",
      "Standard deviation: 0.049\n",
      "\n",
      "Recall: 0.880\n",
      "95% Confidence Interval +/- 0.047\n",
      "Standard deviation: 0.024\n",
      "\n",
      "Accuracy: 0.928\n",
      "95% Confidence Interval +/- 0.031\n",
      "Standard deviation: 0.016\n",
      "\n",
      "=====> Scoring HTTPCLIENT <=====\n",
      "F1score: 0.808\n",
      "95% Confidence Interval +/- 0.059\n",
      "Standard deviation: 0.030\n",
      "\n",
      "Precision: 0.896\n",
      "95% Confidence Interval +/- 0.106\n",
      "Standard deviation: 0.053\n",
      "\n",
      "Recall: 0.738\n",
      "95% Confidence Interval +/- 0.076\n",
      "Standard deviation: 0.038\n",
      "\n",
      "Accuracy: 0.857\n",
      "95% Confidence Interval +/- 0.045\n",
      "Standard deviation: 0.022\n",
      "\n",
      "=====> Scoring LUCENE <=====\n",
      "F1score: 0.870\n",
      "95% Confidence Interval +/- 0.092\n",
      "Standard deviation: 0.046\n",
      "\n",
      "Precision: 0.930\n",
      "95% Confidence Interval +/- 0.082\n",
      "Standard deviation: 0.041\n",
      "\n",
      "Recall: 0.821\n",
      "95% Confidence Interval +/- 0.138\n",
      "Standard deviation: 0.069\n",
      "\n",
      "Accuracy: 0.931\n",
      "95% Confidence Interval +/- 0.046\n",
      "Standard deviation: 0.023\n",
      "\n",
      "=====> Scoring JCR <=====\n",
      "F1score: 0.882\n",
      "95% Confidence Interval +/- 0.043\n",
      "Standard deviation: 0.021\n",
      "\n",
      "Precision: 0.912\n",
      "95% Confidence Interval +/- 0.060\n",
      "Standard deviation: 0.030\n",
      "\n",
      "Recall: 0.855\n",
      "95% Confidence Interval +/- 0.041\n",
      "Standard deviation: 0.020\n",
      "\n",
      "Accuracy: 0.911\n",
      "95% Confidence Interval +/- 0.033\n",
      "Standard deviation: 0.016\n",
      "\n",
      "=> End run setting 5 <=\n"
     ]
    }
   ],
   "source": [
    "raw_data = load_data()\n",
    "run_setting_5(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save fitted models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def save_fitted_models(raw_data):\n",
    "    print(\"=> Run save fitted models <=\")\n",
    "    print(\"Get corpus and labels for all projects\")\n",
    "    #stemmer = SnowballStemmer(\"english\")\n",
    "    stemmer = None\n",
    "    corpus, labels = get_corpus_labels(raw_data,stemmer)\n",
    "    vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, ngram_range=(1, 3), sublinear_tf=True, stop_words={'english'})\n",
    "\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(15, 9, 10, 11, 9, 15, 11), activation='tanh', learning_rate='adaptive',\n",
    "                        max_iter=100, random_state=0)\n",
    "\n",
    "    X, vectorizer, ch2 = feature_computing(corpus, labels, vectorizer, k_best=37362)\n",
    "    binarized_labels = labels_binarizing(labels)\n",
    "    \n",
    "    #mlp.fit(X,binarized_labels)\n",
    "    #dump(mlp, \"mlp_model.pkl\")\n",
    "    dump(vectorizer, \"vectorizer_model.pkl\")\n",
    "    dump(ch2,\"ch2_model.pkl\")\n",
    "\n",
    "    print(\"=> End run setting 5 <=\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Run save fitted models <=\n",
      "Get corpus and labels for all projects\n",
      "1940 BUG / 5591 \n",
      "\n",
      "Feature computing.\n",
      "\t99349 features.\n",
      "Extracting 37362 best features by a chi-squared test\n",
      "=> End run setting 5 <=\n"
     ]
    }
   ],
   "source": [
    "raw_data = load_data()\n",
    "save_fitted_models(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "save_fitted_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "save_fitted_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_fitted_models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}